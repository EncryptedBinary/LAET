{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebea9f-e9fb-4b83-b9d6-f1e5a244e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch pandas tqdm transformers datasets trl peft bitsandbytes scikit-learn openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7803c4-eab5-4be8-93af-602273d156cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58836e4-e760-43e8-9f0b-0ca94b91114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('Dataset_Name')\n",
    "\n",
    "# Dataset features\n",
    "train_data = dataset['train']\n",
    "valid_data = dataset['valid']\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355ad68-cb5c-444f-aac3-3f413b1ef901",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abe14b90-7ce8-488a-9e19-1ef9b2756c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Analyze the sentiment of this statement extracted from a financial news article. Provide your answer as either negative, positive, or neutral.\\nText: The five-storey , eco-efficient building will have a gross floor area of about 15,000 sq m. It will also include apartments .\\nAnswer:'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['query'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145afced-fc20-463d-bf0f-93b7ee2488c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c5df832-8e7a-4b2b-84d6-a2a85479399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(example):\n",
    "    return f\"\"\"{example['query']} {example['answer']}\"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(example):\n",
    "    return f\"\"\" {example['query']} \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d68abe36-bb95-466f-a45e-a1d9b7e4acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_data.to_pandas()\n",
    "test_df = test_data.to_pandas()\n",
    "val_df = valid_data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f4392e3-25f2-48ea-870f-cce5abe4cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=test_df['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dbf21a9-9b8e-4e4a-af0b-c8a877d633a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['prompt'] = train_df.apply(generate_prompt, axis=1)\n",
    "test_df['prompt'] = test_df.apply(generate_test_prompt, axis=1)\n",
    "val_df['prompt'] = val_df.apply(generate_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b134809-9d42-400c-b6b4-e3c17bc5ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_pandas(train_df[['prompt']])\n",
    "valid_data = Dataset.from_pandas(val_df[['prompt']])\n",
    "test_data = Dataset.from_pandas(test_df[['prompt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bba179c4-687c-4bba-9fd1-8a7fb424074f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Analyze the sentiment of this statement extracted from a financial news article. Provide your answer as either negative, positive, or neutral.\\nText: The five-storey , eco-efficient building will have a gross floor area of about 15,000 sq m. It will also include apartments .\\nAnswer: neutral'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e90e2ea-72a6-4d79-a684-a31d0d846cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt'],\n",
       "    num_rows: 3100\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5b355-dab5-4def-bcb2-0a1cdc110a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"Model_Name\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177e101-7cf1-4ca2-acf2-fc7a5047a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['prompt'])):\n",
    "        output_texts.append(example['prompt'][i])\n",
    "\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "response_template = \"### Answer:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28047a92-5461-4f2a-a0ce-fb2764af421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test[\"prompt\"][i]\n",
    "        pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer,\n",
    "                        max_new_tokens = 7, \n",
    "                        temperature = 0.1,\n",
    "                       )\n",
    "        result = pipe(prompt, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "        # print(result)\n",
    "        answer = result[0]['generated_text'].split(\"Answer:\")[-1].lower()\n",
    "        # print(f\"answer: {answer}\")\n",
    "        if \"neutral\" in answer:\n",
    "            y_pred.append(\"neutral\")\n",
    "        elif \"positive\" in answer:\n",
    "            y_pred.append(\"positive\")\n",
    "        else: \n",
    "            y_pred.append(\"negative\")\n",
    "\n",
    "        # print(f\"y_pred{y_pred}\")\n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict(test_data, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383a068-ed7f-4dce-b6cd-38f8cdee092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Overall Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy for each label\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        # Get true and predicted values for the current label\n",
    "        true_label = [1 if y == label else 0 for y in y_true]\n",
    "        pred_label = [1 if y == label else 0 for y in y_pred]\n",
    "        label_accuracy = accuracy_score(true_label, pred_label)\n",
    "        print(f'Accuracy for label \"{label}\": {label_accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "  \n",
    "# Assuming y_true and y_pred are your true and predicted labels\n",
    "evaluate(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b633e3eb-31e6-4a98-bd5c-f3160082ae56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41f2071a-7584-4430-b212-e9f2d526462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class CustomEvalCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        print(\"\\nEvaluating model...\\n\")\n",
    "        \n",
    "        # Generate predictions on X_test\n",
    "        y_pred = predict(test_data, model, tokenizer)\n",
    "        \n",
    "        # Evaluate predictions\n",
    "        evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49f95b-4360-44fa-86b0-8e6f214e85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "output_dir=\"out\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules,\n",
    ")\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=10,                       # number of training epochs\n",
    "    per_device_train_batch_size=4,            # batch size per device during training\n",
    "    gradient_accumulation_steps=1,            # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=1,                         \n",
    "    learning_rate=2e-5,                       # learning rate, based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    "    fp16 = False,\n",
    "    bf16 = False,\n",
    "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
    "    # report_to=\"wandb\",                  # report metrics to w&b\n",
    "    eval_strategy=\"epoch\",              # save checkpoint every epoch\n",
    "    eval_steps = 0.2,            # perform evaluation at the end of each epoch\n",
    "    save_strategy=\"epoch\",                    # save model checkpoint at the end of each epoch\n",
    "    # save_total_limit=3                        # limit the total number of saved checkpoints to avoid storage issues\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b49d1-5c12-4635-af80-2b1a7b6efc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    # tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    "    peft_config=peft_config,\n",
    "    #dataset_text_field=\"Sentiment\",\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    # callbacks=[CustomEvalCallback()] \n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727d737-a0ba-418e-b23c-0e14497487b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f073a-a28d-4425-86a0-8942932dc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Generate predictions on X_test\n",
    "y_pred = predict(test_data, model, tokenizer)\n",
    "        \n",
    "        # Evaluate predictions\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2f86e-6b70-4296-b73c-484714481304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
